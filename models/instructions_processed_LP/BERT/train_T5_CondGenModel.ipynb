{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cz/.local/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "no_appended = False # True if you want to use the no_appended (high level)\n",
    "learning_rate = 1e-5\n",
    "decay = 0.9\n",
    "decay_term = 9\n",
    "task = 'object'\n",
    "model_type = 't5-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a new labelling dataset that matches each obj label to the splitted obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Alarm Clock',\n",
       " 1: 'Aluminum Foil',\n",
       " 2: 'Apple',\n",
       " 3: 'Apple Sliced',\n",
       " 4: 'Arm Chair',\n",
       " 5: 'Baseball Bat',\n",
       " 6: 'Basket Ball',\n",
       " 7: 'Bathtub',\n",
       " 8: 'Bathtub Basin',\n",
       " 9: 'Bed',\n",
       " 10: 'Blinds',\n",
       " 11: 'Book',\n",
       " 12: 'Boots',\n",
       " 13: 'Glassbottle',\n",
       " 14: 'Bowl',\n",
       " 15: 'Box',\n",
       " 16: 'Bread',\n",
       " 17: 'Bread Sliced',\n",
       " 18: 'Butter Knife',\n",
       " 19: 'Cabinet',\n",
       " 20: 'Candle',\n",
       " 21: 'CD',\n",
       " 22: 'Cell Phone',\n",
       " 23: 'Chair',\n",
       " 24: 'Cloth',\n",
       " 25: 'Coffee Machine',\n",
       " 26: 'Coffee Table',\n",
       " 27: 'Counter Top',\n",
       " 28: 'Credit Card',\n",
       " 29: 'Cup',\n",
       " 30: 'Curtains',\n",
       " 31: 'Desk',\n",
       " 32: 'Desk Lamp',\n",
       " 33: 'Desktop',\n",
       " 34: 'Dining Table',\n",
       " 35: 'Dish Sponge',\n",
       " 36: 'Dog Bed',\n",
       " 37: 'Drawer',\n",
       " 38: 'Dresser',\n",
       " 39: 'Dumbbell',\n",
       " 40: 'Egg',\n",
       " 41: 'Egg Cracked',\n",
       " 42: 'Faucet',\n",
       " 43: 'Floor',\n",
       " 44: 'Floor Lamp',\n",
       " 45: 'Footstool',\n",
       " 46: 'Fork',\n",
       " 47: 'Fridge',\n",
       " 48: 'Garbage Bag',\n",
       " 49: 'Garbage Can',\n",
       " 50: 'Hand Towel',\n",
       " 51: 'Hand Towel Holder',\n",
       " 52: 'House Plant',\n",
       " 53: 'Kettle',\n",
       " 54: 'Key Chain',\n",
       " 55: 'Knife',\n",
       " 56: 'Ladle',\n",
       " 57: 'Laptop',\n",
       " 58: 'Laundry Hamper',\n",
       " 59: 'Lettuce',\n",
       " 60: 'Lettuce Sliced',\n",
       " 61: 'Light Switch',\n",
       " 62: 'Microwave',\n",
       " 63: 'Mirror',\n",
       " 64: 'Mug',\n",
       " 65: 'Newspaper',\n",
       " 66: 'Ottoman',\n",
       " 67: 'Painting',\n",
       " 68: 'Pan',\n",
       " 69: 'Paper Towel',\n",
       " 70: 'Pen',\n",
       " 71: 'Pencil',\n",
       " 72: 'Pepper Shaker',\n",
       " 73: 'Pillow',\n",
       " 74: 'Plate',\n",
       " 75: 'Plunger',\n",
       " 76: 'Poster',\n",
       " 77: 'Pot',\n",
       " 78: 'Potato',\n",
       " 79: 'Potato Sliced',\n",
       " 80: 'Remote Control',\n",
       " 81: 'Room Decor',\n",
       " 82: 'Safe',\n",
       " 83: 'Salt Shaker',\n",
       " 84: 'Scrub Brush',\n",
       " 85: 'Shelf',\n",
       " 86: 'Shelving Unit',\n",
       " 87: 'Shower Curtain',\n",
       " 88: 'Shower Door',\n",
       " 89: 'Shower Glass',\n",
       " 90: 'Shower Head',\n",
       " 91: 'Side Table',\n",
       " 92: 'Sink',\n",
       " 93: 'Sink Basin',\n",
       " 94: 'Soap Bar',\n",
       " 95: 'Soap Bottle',\n",
       " 96: 'Sofa',\n",
       " 97: 'Spatula',\n",
       " 98: 'Spoon',\n",
       " 99: 'Spray Bottle',\n",
       " 100: 'Statue',\n",
       " 101: 'Stool',\n",
       " 102: 'Stove Burner',\n",
       " 103: 'Stove Knob',\n",
       " 104: 'Table Top Decor',\n",
       " 105: 'Target Circle',\n",
       " 106: 'Teddy Bear',\n",
       " 107: 'Television',\n",
       " 108: 'Tennis Racket',\n",
       " 109: 'Tissue Box',\n",
       " 110: 'Toaster',\n",
       " 111: 'Toilet',\n",
       " 112: 'Toilet Paper',\n",
       " 113: 'Toilet Paper Hanger',\n",
       " 114: 'Tomato',\n",
       " 115: 'Tomato Sliced',\n",
       " 116: 'Towel',\n",
       " 117: 'Towel Holder',\n",
       " 118: 'TVStand',\n",
       " 119: 'Vacuum Cleaner',\n",
       " 120: 'Vase',\n",
       " 121: 'Watch',\n",
       " 122: 'Watering Can',\n",
       " 123: 'Window',\n",
       " 124: 'Wine Bottle',\n",
       " 125: 'None'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "template_by_label = pickle.load(open('data/alfred_data/alfred_dicts/template_by_label.p', 'rb'))\n",
    "train_set = pickle.load(open('data/alfred_data/train_text_with_ppdl_low_appended.p', 'rb'))\n",
    "val_set_seen = pickle.load(open('data/alfred_data/val_seen_text_with_ppdl_low_appended.p', 'rb'))\n",
    "val_set_unseen = pickle.load(open('data/alfred_data/val_unseen_text_with_ppdl_low_appended.p', 'rb'))\n",
    "\n",
    "obj2idx = pickle.load(open('data/alfred_data/alfred_dicts/obj2idx.p', 'rb'))\n",
    "idx2obj = pickle.load(open('data/alfred_data/alfred_dicts/idx2obj.p', 'rb'))\n",
    "idx2recep = pickle.load(open('data/alfred_data/alfred_dicts/idx2recep.p', 'rb'))\n",
    "recep2idx = pickle.load(open('data/alfred_data/alfred_dicts/recep2idx.p', 'rb'))\n",
    "toggle2idx = pickle.load(open('data/alfred_data/alfred_dicts/toggle2idx.p', 'rb'))\n",
    "\n",
    "label_arg = 'object_targets'; num_labels = len(obj2idx) # 127\n",
    "idx2obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarize the task parameters: move the alarm clock from one desk to another one\n",
      "summarize the task parameters: move the alarm clock from one desk to another one[SEP]turn right turn left walk towards the small desk on the left side[SEP]pick up the alarm clock that is on the desk[SEP]turn right walk towards the wall turn to face the desk to the left[SEP]put the alarm clock on the desk\n",
      "task_type: pick_and_place_simple, object_target: Alarm Clock, mrecep_target: None, parent_target: Desk\n"
     ]
    }
   ],
   "source": [
    "train_prefix_high = train_set['x_prefix']\n",
    "train_prefix_low = train_set['x_low_prefix']\n",
    "label_parameters = train_set['parameters']\n",
    "print(train_prefix_high[88])\n",
    "print(train_prefix_low[88])\n",
    "print(label_parameters[88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize and encode the labels for each object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape torch.Size([21025, 41])\n",
      "labels shape torch.Size([21025, 48])\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(train_prefix_high, return_tensors='pt', padding='longest', truncation=True).input_ids.to(device) # tokenized input\n",
    "labels = tokenizer(label_parameters, return_tensors='pt', padding='longest', truncation=True).input_ids.to(device) # tokenized labels\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "print(\"input_ids shape\", input_ids.shape)\n",
    "print(\"labels shape\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do the same for validation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarize the task parameters: place a vase on a coffee table\n",
      "summarize the task parameters: place a vase on a coffee table[SEP]turn right and cross the room turn right  and walk to the tc stand on your left turn to face the tv stand[SEP]pick up the blue vase that is sitting on the tv stand [SEP]turn around to face the black coffee table in[SEP]place the vase on the coffee table to the left of the computer\n",
      "task_type: pick_and_place_simple, object_target: Vase, mrecep_target: None, parent_target: Coffee Table\n"
     ]
    }
   ],
   "source": [
    "val_seen_prefix_high = val_set_seen['x_prefix']\n",
    "val_seen_prefix_low = val_set_seen['x_low_prefix']\n",
    "val_seen_label_parameters = val_set_seen['parameters']\n",
    "print(val_seen_prefix_high[67])\n",
    "print(val_seen_prefix_low[67])\n",
    "print(val_seen_label_parameters[67])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "input_ids_val_seen = tokenizer(val_seen_prefix_high, return_tensors='pt', padding='longest', truncation=True).input_ids.to(device) # tokenized val seen input\n",
    "labels_val_seen = tokenizer(val_seen_label_parameters, return_tensors='pt', padding='longest', truncation=True).input_ids.to(device) # tokenized labels\n",
    "labels_val_seen[labels_val_seen == tokenizer.pad_token_id] = -100\n",
    "\n",
    "val_seen_dataset = TensorDataset(input_ids_val_seen, labels_val_seen)\n",
    "val_seen_loader = DataLoader(val_seen_dataset, batch_size=100)  # Batch size set to 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cz/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "print('Model initialized successfully')\n",
    "model.train()\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "if 'base' in model_type:\n",
    "    N= 32\n",
    "else:\n",
    "    N = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs_batch, labels_batch):\n",
    "    y_hat = outputs_batch.argmax(dim=-1)\n",
    "    batch_acc = (y_hat == labels_batch).float().mean()\n",
    "    return batch_acc # returns the accuracy of the batch\n",
    "\n",
    "# def accurate_total(y_pred, y_batch): #calculates the number of accurate predictions \n",
    "#     y_hat = torch.argmax(y_pred, dim=-1)\n",
    "#     correct_predictions = (y_hat == y_batch).float()\n",
    "\n",
    "#     # Count the total number of correct predictions\n",
    "#     num_accurate = correct_predictions.sum().item()\n",
    "\n",
    "#     return num_accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape:  torch.Size([21025, 41])\n",
      "input_ids_batch shape:  torch.Size([32, 41])\n",
      "labels_batch shape:  torch.Size([32, 48])\n",
      "labels_batch:  tensor([[2491,  834, 6137,  ..., -100, -100, -100],\n",
      "        [2491,  834, 6137,  ..., -100, -100, -100],\n",
      "        [2491,  834, 6137,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [2491,  834, 6137,  ..., -100, -100, -100],\n",
      "        [2491,  834, 6137,  ..., -100, -100, -100],\n",
      "        [2491,  834, 6137,  ..., -100, -100, -100]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 3.81 GiB total capacity; 2.85 GiB already allocated; 11.12 MiB free; 2.98 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6e4248443147>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0maccuracy_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mavg_training_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m         )\n\u001b[1;32m   1652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 )\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         )\n\u001b[1;32m    677\u001b[0m         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpresent_key_value_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         )\n\u001b[1;32m    583\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 3.81 GiB total capacity; 2.85 GiB already allocated; 11.12 MiB free; 2.98 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "accuracy_dictionary = {'training_loss': [], 'training':[], 'val_seen_loss': [],'val_seen_acc':[]}\n",
    "start_train_time = time.time()\n",
    "\n",
    "for t in range(50):\n",
    "    model.train()\n",
    "    if t>0 and (t+1)%decay_term ==0:\n",
    "        learning_rate *= decay\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    avg_training_loss = 0.0\n",
    "    avg_training_acc = 0.0\n",
    "    for b in range(int(input_ids.shape[0]/N)):\n",
    "        print('input_ids shape: ', input_ids.shape)\n",
    "        input_ids_batch = input_ids[N*b:N*(b+1)].to(device) # has shape [64, 475]\n",
    "        print('input_ids_batch shape: ', input_ids_batch.shape)\n",
    "        labels_batch = labels[N*b:N*(b+1)].to(device)\n",
    "        optimizer.zero_grad() # clear gradients\n",
    "        print('labels_batch shape: ', labels_batch.shape)\n",
    "        print('labels_batch: ', labels_batch)\n",
    "\n",
    "        #forward pass\n",
    "        outputs = model(input_ids_batch, labels=labels_batch)\n",
    "        accuracy_batch = accuracy(outputs.logits, labels_batch)\n",
    "        avg_training_acc += accuracy_batch.item()\n",
    "\n",
    "        loss = outputs.loss\n",
    "        avg_training_loss += loss.item()\n",
    "        #if t ==0:\n",
    "        #    print(\"loss at step \", t, \" : \", loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_training_loss *= 1/int(input_ids.shape[0]/N)\n",
    "    avg_training_acc *= 1/int(input_ids.shape[0]/N)\n",
    "    accuracy_dictionary['training_loss'].append(avg_training_loss)\n",
    "    accuracy_dictionary['training'].append(avg_training_acc)\n",
    "    #Print & Evaluate\n",
    "    #evaluate\n",
    "    model.eval()\n",
    "    avg_val_seen_loss = 0.0\n",
    "    avg_val_seen_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_seen_loader:\n",
    "            val_seen_input_ids_batch, val_seen_labels_batch = batch\n",
    "\n",
    "            outputs_val_seen = model(val_seen_input_ids_batch, labels=val_seen_labels_batch)\n",
    "            val_accuracy_batch = accuracy(outputs_val_seen.logits, val_seen_labels_batch)\n",
    "            avg_val_seen_acc += val_accuracy_batch.item()\n",
    "\n",
    "            val_seen_loss = outputs_val_seen.loss\n",
    "            avg_val_seen_loss += val_seen_loss.item()\n",
    "\n",
    "        avg_val_seen_acc *= 1/len(val_seen_loader)\n",
    "        avg_val_seen_loss *= 1/len(val_seen_loader)\n",
    "        accuracy_dictionary['val_seen_acc'].append(avg_val_seen_acc)\n",
    "        accuracy_dictionary['val_seen_loss'].append(avg_val_seen_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
